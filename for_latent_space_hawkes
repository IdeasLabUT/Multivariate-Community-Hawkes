    elif dataset == "fb-forum":
        data_path = "/nethome/hsolima/MultivariateBlockHawkesProject/MultivariateBlockHawkes/storage/datasets"
        save_path = "/shared/Results/MultiBlockHawkesModel/LSH_tests"
        train_path = "fb-forum/fb_forum_train.csv"
        test_path = "fb-forum/fb_forum_test.csv"
        # read source, target, timestamp of train dataset - ignore heades
        train_data = np.loadtxt(f"{data_path}/{train_path}", delimiter=",", skiprows=1, usecols=[0, 1, 2])
        test = np.loadtxt(f"{data_path}/{test_path}", delimiter=",", skiprows=1, usecols=[0, 1, 2])

        # train node id map
        nodes_train_set = set(np.r_[train_data[:, 0], train_data[:, 1]])
        node_id_map = {}
        for i, n in enumerate(nodes_train_set):
            node_id_map[n] = i

        # create events_dict_train
        events_dict_train = {}
        for i in range(len(train_data)):
            sender_id, receiver_id = node_id_map[train_data[i,0]], node_id_map[train_data[i,1]]
            if (sender_id, receiver_id) not in events_dict_train:
                events_dict_train[(sender_id, receiver_id)] = []
            events_dict_train[(sender_id, receiver_id)].append(train_data[i,2])

        # remove nodes in test not in train
        n_events_test = 0
        events_dict_all = copy.deepcopy(events_dict_train)
        for i in range(len(test)):
            if test[i, 0] in node_id_map and test[i, 1] in node_id_map:
                n_events_test +=1
                sender_id, receiver_id = node_id_map[test[i,0]], node_id_map[test[i,1]]
                if (sender_id, receiver_id) not in events_dict_all:
                    events_dict_all[(sender_id, receiver_id)] = []
                events_dict_all[(sender_id, receiver_id)].append(test[i, 2])
        nodes_not_in_train = []
        n_nodes_train = n_nodes_all = len(nodes_train_set)
        T_train = train_data[-1, 2]
        T_all = test[-1, 2]
        n_events_train = len(train_data)
        n_events_all = n_events_train + n_events_test
        n_train_events = 0
        for (u, v) in events_dict_train:
            n_train_events += len(events_dict_train[(u, v)])
        print(n_train_events)


    elif dataset == "Enron-15":
        save_path = "/shared/Results/MultiBlockHawkesModel/LSH_tests"
        with open('storage/datasets/enron2/enron-events.pckl', 'rb') as f:
            n_nodes_all, T_all, enron_all = pickle.load(f)
        n_nodes_train = n_nodes_all
        T_train = 316

        events_dict_train = {}
        events_dict_all = {}
        n_events_train = 0
        n_events_all = len(enron_all)
        for u, v, t in enron_all:
            if t <= T_train:
                n_events_train += 1
                if (u, v) not in events_dict_train:
                    events_dict_train[(u, v)] = []
                events_dict_train[(u, v)].append(t)
            if (u, v) not in events_dict_all:
                events_dict_all[(u, v)] = []
            events_dict_all[(u, v)].append(t)
        nodes_not_in_train = []